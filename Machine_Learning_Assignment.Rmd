---
title: "Machine Learning Assignment"
author: "John Howell"
date: "October 9, 2016"
output: html_document
---

```{r echo=FALSE, message=FALSE, error=FALSE}
setwd("~/Documents/My Documents/Data Courses/Machine Learning")
library(caret)
library(ggplot2)
library(randomForest)
library(rattle)
library(kernlab)
library(ISLR)
library(rpart)
library(gbm)
library(ElemStatLearn)
library(e1071)

fileUrl1<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileUrl2<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

download.file(fileUrl1, destfile = "dlTrain.csv", method = "curl")
download.file(fileUrl2, destfile = "dlTest.csv", method = "curl")

dlTrain<-read.csv("dlTrain.csv")
dlTest<-read.csv("dlTest.csv")
```
## Executive Summary: 
### The purpose of this exercise is to use predictive machine learning to predict how well a group of individuals performed a specific exercise.  Devices such as Jawbone "Up", Nike "FuelBand" and Fitbit are making it possible to collect a large amount of data about personal activity.  People who use these devices regularly quantify how much of a particular activity they do, but they rarely quantify how well they do it.  In a study performed by Velloso, Bulling, Gellersen, Ugulino and Fuks, (see references), it does provide a measure, "classe", (among many other variables) of how well 6 males, each perform one set of 10 repetitions of the Unilateral Dumbbell Bicep Curl in 5 different fashions: exactly according to specification (class A), throwing the elbows to the front (class B), lifting the dumbbell only halfway (class C), lowering the dumbbell only halfway (calls D) and throwing the hips to the front (class E).  

## Data: 
### The dataset referenced above is defined as the "training" dataset which the author will use to train predictive models against.  The overall training dataset will be divided into a training dataset and a cross-validation set.  Once the models have been trained against the training dataset and the most accurate method selected, then that method will be used against the cross-validation set to determine the predictive accuracy.  Given that the accuracy remains high (95% or better) then, and only then, will the model be used against the actual "test" data set.

## Cleaning the data: 
### By looking at the training data there are multiple columns which either do not contain numerical data (ie "N/A", " ", or 0) or data that are not applicable to the model (the first 7 columns).  All of these columns need to be eliminated from the final training data frame.  The testing data will require the same cleaning method.  The following histogram is a snap-shot of the training data:
```{r echo=TRUE, message=FALSE, error=FALSE}
#Clean training data
trainnew <- data.frame(dlTrain)
trainnew <-trainnew[ , ! apply(trainnew, 2, function(x) any(is.na(x)))]
trainnew <-trainnew[ , ! apply(trainnew, 2, function(x) any(x == ""))]
trainnew <-trainnew[ , ! apply(trainnew, 2, function(x) all(x == 0))]
trainnew <-trainnew[ , -(1:7), drop=FALSE]
g<-as.numeric(trainnew$classe)
hist(g, main="Histogram of Classe Data", xlab="Classe (1=A, 2=B, 3=C, 4=D, 5=E)", border="blue", col="green", breaks=c(1,2,3,4,5))
```

```{r echo=FALSE}
#Clean testing data
testnew <- data.frame(dlTest)

testnew <-testnew[ , ! apply(testnew, 2, function(x) any(is.na(x)))]
testnew <-testnew[ , ! apply(testnew, 2, function(x) any(x == ""))]
testnew <-testnew[ , ! apply(testnew, 2, function(x) all(x == 0))]
testnew <-testnew[ , -(1:7), drop=FALSE]
```

## Conduct Data Splitting: 
### In order to begin the model-training process we need to randomly split the training data set into training ("training") and cross-validation ("cross_val").  I use a 75/25% split.  
```{r echo=FALSE, message=FALSE, error=FALSE}
inTrain <- createDataPartition(y=trainnew$classe, p=0.75, list=FALSE)
training <- trainnew[inTrain, ]
cross_val <- trainnew[-inTrain, ]
```

## Fit the model/ Train the model: 
### This step utilizes both "set.seed" and cross-validation processes, as well as "train" and "predict" functions to determine the most accurate model to utilize.  The models which are evaluated against each other here are the "Rpart", "Random Forest" and "GBM".
```{r echo=FALSE, message=FALSE, error=FALSE}
set.seed(724)

#Cross validation
fitTrn <- trainControl(method="cv", number=3, verboseIter=F)

#Evaluate model on training dataset (Predictions and confusion matrices)
#Rpart
modelRpart<- train(classe ~ ., data=training, method="rpart", trControl=fitTrn)
prdRpart<-predict(modelRpart, newdata=training)
cmRpart<-confusionMatrix(prdRpart, training$classe)
accRpart<-cmRpart$overall["Accuracy"]

#rf
modelRf<-train(classe ~ ., data=training, method="rf", trControl=fitTrn)
prdRf<-predict(modelRf, newdata=training)
cmRf<-confusionMatrix(prdRf, training$classe)
accRf<-cmRf$overall["Accuracy"]

#gbm
modelGbm<-train(classe ~ ., data=training, method="gbm", trControl=fitTrn, verbose=FALSE)
prdGbm<-predict(modelGbm, newdata=training)
cmGbm<-confusionMatrix(prdGbm, training$classe)
accGbm<-cmGbm$overall["Accuracy"]
```
## The accuracies of the models are as follows:
### R Part: `r accRpart`
### Random Forest: `r accRf`
### GBM: `r accGbm`
### Therefore, the Random Forest model appears to be the most accurate and will be used from here on.
## Predict the Out-of-Sample Error:
### This prediction entails using the RF model, cross-validation and the "predict" function to determine the model's accuracy and out-of-sample error against the cross-validation dataset.
```{r echo=FALSE, message=FALSE, error=FALSE}
#RF appears to be most accurate.  Use it to evaluate model on testing dataset 
modelRftest<-train(classe ~ ., data=cross_val, method="rf", trControl=fitTrn)
prdtstRf<-predict(modelRftest, newdata=cross_val)
cmRftest<-confusionMatrix(prdtstRf, cross_val$classe)
print(cmRftest)

#accuracy of prediction
accRF<-cmRftest$overall["Accuracy"]

Ooserror<-1-accRF
```
### The accuracy of the prediction of out-of-sample error is `r accRF`.
### The out-of-sample error is `r Ooserror`
### Importance of variables in the model must be known for further regressive analysis and to determine the highest level of correlation between which variables and the variable (classe) that we want to predict. 
```{r echo=FALSE, message=FALSE, error=FALSE}
#Importance of variables- what are the 5 most important variables??
variableimp<- varImp(modelRftest)$importance
print("The top 5 variables in importance:")
print(variableimp[head(order(unlist(variableimp), decreasing = TRUE), 5L), , drop = FALSE])
```
### These results show high enough accuracy that we can use this same model/ method against the original test data set.  This test data set does not contain any "Classe" variables and the predictive machine learning model will give predicted values for classe.  These predicted values are as follows:
```{r echo=FALSE, message=FALSE, error=FALSE}
#RF appears to be most accurate.  Use it to evaluate model on testing dataset 
#modelRftest<-train(classe ~ ., data=cross_val, method="rf", trControl=fitTrn)
prdtstRf<-predict(modelRftest, newdata=testnew)
#cmRftest<-confusionMatrix(prdtstRf, testnew$classe)
print(prdtstRf)
```
### References: 
#### Velloso, E; Bulling, A; Gellersen, H; Ugulino, W; Fuks, H. "Qualitative Activity Recognition of Weight Lifting Exercises", Proceedings of 4th International Conference in Cooperation with SIGHI. Stuttgart, GE, 2013
